{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition Using K Nearest Neighbors\n",
    "\n",
    "We've briefly gone over using K Nearest Neighbors in Python.\n",
    "\n",
    "Let's go more in depth this time, as well as introduce basic cross validation. Cross validation will give us a more generalized measure of the model's scores, as well as help us to automatically tune the model (i.e. select the best `k` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What kind of object is digits?\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print digits.DESCR #The description of the dataset\n",
    "print digits.images.shape # The formatted array. This is an nparray of 1797 x 8 x 8 (1797 images having 8x8 pixels)\n",
    "print digits.data.shape #The raw data. This is an nparray of size 1797 x 64. Imagine each 8x8 image row's stacked into a single vector\n",
    "\n",
    "print digits.target.shape\n",
    "print digits.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits.images[0] #If you squint your eyes hard enough, the non zero values look roughly like a \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits.data[0] #Same as above, but each row has been stacked into a single vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Let's visualize what these images look like\n",
    "#plt.imshow( digits.images[0])\n",
    "plt.imshow( digits.images[0], cmap=plt.cm.gray_r) #Color map, or cmap provides a mapping between the values, and the represented colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lets look at a couple of these\n",
    "for index in range(5):\n",
    "    image = digits.images[index]\n",
    "    label = digits.target[index]\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.subplot(2,3,index+1)\n",
    "    plt.imshow(image,cmap=plt.cm.gray_r)\n",
    "    plt.title(\"Image label: %i\" % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's see if we can do a decent job of classifying digits. What we will do is, we will consider each row of digits.data to be a sample. \n",
    "\n",
    "X = digits.data, where the columns are features (i.e. pixel values), and the rows are samples\n",
    "y = digits.target, where each element is the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sklearn is the package for machine learning in python.\n",
    "# It has a host of machine learning tools/models, and various datasets\n",
    "\n",
    "# http://scikit-learn.org/stable/ \n",
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = digits.images.shape[0]\n",
    "X         = digits.data\n",
    "y         = digits.target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just created our entire dataset. Ideally, what we would like to do before building a model, is to split this dataset into a training and testing dataset.\n",
    "\n",
    "```\n",
    "X = [ 0 1 4 6 8 8 ... 9 ]\n",
    "    [ 1 5 7 2 0 0 ... 3 ]\n",
    "    [ 4 3 5 9 8 0 ... 0 ]\n",
    "    [ 3 5 0 0 1 1 ... 7 ]\n",
    "    [ 4 4 9 3 9 4 ... 1 ]  \n",
    "    [ 0 4 2 4 1 1 ... 1 ]\n",
    "    \n",
    "split to...\n",
    "\n",
    "X = [ 0 1 4 6 8 8 ... 9 ]\n",
    "    [ 1 5 7 2 0 0 ... 3 ]  Training Data Set\n",
    "    [ 4 3 5 9 8 0 ... 0 ]\n",
    "    [ 3 5 0 0 1 1 ... 7 ]\n",
    "    ---------------------\n",
    "    [ 4 4 9 3 9 4 ... 1 ]  Testing Data Set\n",
    "    [ 0 4 2 4 1 1 ... 1 ]\n",
    "    \n",
    "```\n",
    "\n",
    "```\n",
    "Y = [ 1 ]\n",
    "    [ 0 ]\n",
    "    [ 9 ] \n",
    "    [ 7 ] \n",
    "    [ 3 ]\n",
    "    [ 4 ]\n",
    "    [ 0 ]\n",
    "    \n",
    "split too...\n",
    "\n",
    "Y = [ 1 ]\n",
    "    [ 0 ]\n",
    "    [ 9 ]  Training Labels\n",
    "    [ 7 ] \n",
    "    -----\n",
    "    [ 3 ]\n",
    "    [ 4 ]  Testing Labels\n",
    "    [ 0 ]\n",
    "\n",
    "```\n",
    "\n",
    "Why cant we just select rows 0 to #rows * .8 as training data, and #rows*.8 to #rows as testing data?\n",
    "**When the user inserted or grabbed the original data, it could have been ordered already!.** i.e. What if when mining for image data, the user first grabbed all 0 images, then 1 images, then 2 images, and so on. Then if you split up the dataset, you'll only really have trained on a non-random set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Luckily, there is a function in \"cross_validation\" called \"train_test_split\" that does exactly that for us\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is an example of tuple unpacking. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)  # Use 80% of the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've split up our data, we can 'fit' or 'create' the model using our training data, and score on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The \"fit\" function will be available in nearly all the models that you can import from sklearn\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few parameters here. The ones to note are \"weights\", n_neighbors, and \"metric\" For now, let's use the defaults (which are set to grab distances by euclidean distance).\n",
    "\n",
    "Now that we've fit the model, lets see how the model does in predicting the samples that it trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In predict, you pass in a matrix of samples you want to predict. \n",
    "# For each row in X_train it will return a prediction of the label\n",
    "y_pred = knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well it predicted on the training data. The scoring method for each model will be different, but for knn the scoring metric is #Correct/#Samples (or # Rows correctly predicted / # Total Rows )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(y_pred == y_train) / np.float64(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKlearn also has a `score` function built in. It handles the predction as well as score calculation steps in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There's a function built into \n",
    "knn.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's score on the test data. Pretty good!\n",
    "knn.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of images did the model not do well on?\n",
    "\n",
    "* We can predict using the model on X_test, and save the predictions to y_pred.\n",
    "* Then, we will check the indexes where y_pred != y_test.\n",
    "* Finally, for a few of these, we can plot out the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to index by 0 at the end bc np.where returns a tuple per array dimension\n",
    "inc_idx = np.where(y_pred != y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for plotidx, idx in enumerate(inc_idx):\n",
    "    plt.subplot(1,len(inc_idx),plotidx+1)\n",
    "    plt.imshow(X_test[idx].reshape( (8,8)) ,cmap=plt.cm.gray_r )\n",
    "    plt.xlabel(\"Pred: %i True: %i\" %( y_pred[idx], y_test[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cross Validation\n",
    "\n",
    "The incorrect classifications seem somewhat reasonable. What if we just got lucky with choosing our datasets? If we run the chunk of code multiple times, we just get a different number each time.\n",
    "\n",
    "To the whiteboard for cross validation...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The sklearn cross_validation module also includes a nifty function called cross_val_score\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross_val_score takes in mainly the original model object, the entire dataset X, and the entire labelset y\n",
    "# the argument cv indicates how many folds or partitions to build models for\n",
    "# Return value: Score on the \"test set\" within each partition\n",
    "cv_score = cross_val_score(knn, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Let's see if cross validation can give us a cleaner one step solution for picking the best `k` value.\n",
    "\n",
    "Similar to last time, vary `k` from 1 to 30. With each `k`, create a new KNeighborsClassifier where n_neighbors = k. Then, use cross_val_score() with cv=5, and mean the results of the 5 test partitions. Append this number onto a list.\n",
    "\n",
    "Once you have this list, use pyplot.plot (or plt.plot if you have pyplot aliased), to plot out the trend.\n",
    "\n",
    "Which `k` do you get the best results for?\n",
    "\n",
    "(Hint: Use np.argmax)\n",
    "\n",
    "(Hint 2: If you start with an empty list, remember to add 1 to the argmax output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
