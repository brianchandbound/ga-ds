{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Princeton Salary Datasets\n",
    "- We've learned the theory. Let's get to a real example\n",
    "- In this task, we will study the impact of various factors (education, experience and gender) on salaries in academia.\n",
    "- The dataset is a historic dataset of salaries in an academic department and is easy to study.\n",
    "- According to the website\n",
    "```\n",
    "These are the salary data used in Weisberg's book, consisting of observations on six variables for 52 tenure-track professors in a small college.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "data = pd.read_csv(\"http://data.princeton.edu/wws509/datasets/salary.dat\", sep='\\s+')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we start with exploring the dataset for a little."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data.info(), '\\n'\n",
    "print data.head(), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First note we only have a very limited set of data points (52 rows).\n",
    "\n",
    "As we can read on the [Princeton website](http://data.princeton.edu/wws509/datasets/#salary), these columns mean the following:\n",
    "- sx = Sex: male or female,\n",
    "- rk = Rank: full, associate or assistant professor,\n",
    "- yr = Number of years in current rank,\n",
    "- dg = Highest degree: doctorate or masters,\n",
    "- yd = Number of years since highest degree was earned,\n",
    "- sl = Academic year salary, in dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Distribution\n",
    "\n",
    "Let's view the distribution of salaries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "f = data.sl.hist(bins=20)\n",
    "# f = plt.hist(data.sl, bins=20) #Same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What correlates with salary?\n",
    "\n",
    "Next, we would want to what factors may be correlated with salary.  We would expect that as **experience** or **education** increases, the salary should increases as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2,sharey=True)  #Force both subplots to use the same y axis\n",
    "f = data.plot(kind='scatter', x='yd', y='sl', ax=axes[0], title=\"Years since degree\", figsize=(12,3))\n",
    "f = data.plot(kind='scatter', x='yr', y='sl', ax=axes[1], title=\"Years in current rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most importantly, while using **linear** regression, we should see if the relationship is **linear**.  \n",
    "- Does every increase in X lead to a constant increase in Y?  We may expect for the first 5-10 year of experience this true, but perhaps after 10 year the salaries stop increasing or jump dramatically--Linear regression would be unable to capture these nonlinearities (Unless we transform the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn\n",
    "\n",
    "We will first use the **linear_model** module within sklearn to solve the linear regression coefficients for us.\n",
    "\n",
    "Let's first start with a simple model only trying to predict a professor's salary based on `yr` and `yd` (as well as the intercept)\n",
    "\n",
    "Our model will conceptually be this:\n",
    "\n",
    "$y = intercept + \\beta_1*yr + \\beta_2*yd$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import package and create the LinearRegression object.\n",
    "# At creation of object, note the arguments for \"fit_intercept\" (i.e. include an intercept term or not) \n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create our X matrix of observations, and y vector of responses\n",
    "X = data[ ['yr','yd'] ]\n",
    "y = data[ 'sl' ]\n",
    "\n",
    "print X.head(), '\\n'\n",
    "print 'Salary \\n', y[0:5], '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the model, and score\n",
    "model.fit(X, y)    # Fitting takes the X, y, and estimates for \"beta\". Beta = (X'X)^(-1) * X'y\n",
    "model.score(X, y)  # for regression, this is R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# intercept of model\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# beta/slope per dimension\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these go with each dimension\n",
    "coefs = zip(model.coef_, X.columns)\n",
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so the linear model would be the following\n",
    "print \"sl = %.1f + \" % model.intercept_ + \\\n",
    "    \" + \".join(\"%.1f %s\" % coef for coef in coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an intuition in the equation!\n",
    "\n",
    "* The base salary is almost \\$17K\n",
    "* Each year in the current position adds another \\$500\n",
    "* Each year after your degree adds another \\$200.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Predicting new values is simple once we have fit our model. Pass in a matrix **X** of data points that you would like to predict to the model.predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X) # This is the same as y_pred = X*beta = X * (X'X)^(-1)*X'y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can always manually calculate a prediction based on the input values and beta coefficients.\n",
    "\n",
    "If you have had your degree for 6 years, and you've been in your current position for 2 years, what is your predicted salary?\n",
    "\n",
    "$salary = 16555.7 + 489.3 * yr + 222.3 * yd$\n",
    "\n",
    "$salary = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/ Test\n",
    "Supervised problems, like linear regression, always require a training and a testset, to avoid having overfitted your model. (i.e. if we fit on the entire data set, it's like giving all the answers to a final exam)\n",
    "\n",
    "Similar to our KNN example, lets split up our data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8)\n",
    "\n",
    "model.fit(X_train,y_train) #Fit the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.score(X_train,y_train) #Print how high the R^2 is on the training data\n",
    "print model.score(X_test,y_test)   #Print how high the R^2 is on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not so great numbers, and not that consistent either. This is not that surprising if you realize how little samples we have (52) and how little structure we saw in the plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- We've seen how sklearn handles **fitting**, **predicting**, and **scoring** the models/data. \n",
    "- However, these are actually all things that are simple enough for us to do.\n",
    "\n",
    "We will first have a chance to use the sklearn features to do this for us automatically. Next, we will manually calculate these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "#### SKLearn Implementation\n",
    "\n",
    "**1) First, set up your X and y matrix and vector. For now, use the entire dataset to create X as the Pandas dataframe containing yr and yd, and y as a numpy array containing salary.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Next, fit a linear model using X and y. Print out the `intercept`, `yr`, and `yd` coefficient values.**\n",
    "\n",
    "(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Generate the predicted y values given the fitted model. Save these as y_pred**\n",
    "\n",
    "(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) Generate the $R^2$ value of the model for X and y.**\n",
    "\n",
    "(model.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $R^2$ should roughly be around 57% as seen above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "#### StudentLearn Implementation\n",
    "\n",
    "**5) Set up your X and y matrix and vector.**\n",
    "\n",
    "For now, use the entire dataset to create X as a Pandas dataframe, and y as a numpy array. HOWEVER, when creating X, set the first column as all values of \"1\", second column as the yr values, and third column as the yd values. Adding an artificial column of 1's to our matrix is essentially adding an intercept term into model.\n",
    "\n",
    "**IF you initially use a dictionary to create the dataframe like I did, be aware that Pandas will resort your columns alphabetically!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6) Now your X should have 52 rows and 3 columns, y should be a vector of 52 elements. Now that we have X and y, let's solve for the beta coefficients.**\n",
    "\n",
    "**Recall that $ \\widehat{beta} = {(X^{T}X)}^{-1}X^{T}y $**\n",
    "\n",
    "**Calculate the coefficients now and save as `betas`. Confirm that the coefficients match what you generated in #2 especially the order.**\n",
    "\n",
    "Some helpful Pandas operations: data.transpose(), data.dot()\n",
    "\n",
    "Some helpful Numpy operations: np.dot(), np.transpose(), np.linalg.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Predict values of y for the X matrix you have created. $y_{pred} = X * betas$. Save the prediction as y_pred.**\n",
    "\n",
    "Hint: You may need to reshape the betas into an array of (3,1) in order to properly use np.dot or the Pandas dot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7) Calculate $R^2$. For OLS with an intercept, $R^2 = 1.0 - SSE/SST = \\frac{\\sum{(y_{pred} - y_{true})^2}}{\\sum{(y_{true} - E[y_{true}])^2}}$, where the $E[*]$ indicates taking the mean of the quantity**\n",
    "\n",
    "Flatten your y_pred first before doing calculations. This will prevent issues with numpy broadcasting array shapes and other funky errors -- y_pred.flatten()\n",
    "\n",
    "**Confirm that your $R^2$ value matches the one from #4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'd like to somehow incorporate the categorical features as well, even though they're not numerical values\n",
    "- One way to do this is adding columns for each category, with a boolean flag\n",
    "- An easy off-the-shelves solution is `patsy`, a library for describing statistical models using symbolic formulas and building design matrices. Patsy is very similar to `R`.\n",
    "\n",
    "http://patsy.readthedocs.org/en/latest/overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to try a linear model predicting the salary, with features sex, years in rank, years since graduation, and rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('sl ~ sx + yr + yd + rk', data=data, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now our feature matrix\n",
    "- `Intercept` always contains a one, which offers a convenient way of computing the intercept\n",
    "- `sx[T.male]` is 1 if `sx == 'male'`, and 0 otherwise\n",
    "- `rk[T.associate]` is 1 if `rk == 'associate'`, and 0 otherwise\n",
    "- `rk[T.full]` is 1 if `rk == 'full'`, and 0 otherwise\n",
    "- `yr` is just `yr`\n",
    "- `yd` is just `yd`\n",
    "\n",
    "\n",
    "Note that the two `rk` columns can never be both one. If they're both zero, then `rk == assistant`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that `y` is now a one-column dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y).score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better score for sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print model.intercept_\n",
    "print model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first coefficient is zero, since we already fitted an intercept.\n",
    "\n",
    "(**Advanced: Look up 'rank deficient least squares'. This is what solvers do when there are duplicate columns**)\n",
    "\n",
    "Alternatively, we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False)\n",
    "print model.fit(X, y).score(X, y)\n",
    "print model.intercept_\n",
    "print model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that `model.coef_` is now a matrix, since we gave the model a dataframe `y` instead of a series. It is possible to fit more `y`s at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefs = zip(model.coef_[0], X.columns)\n",
    "print \"sl = \" + \" + \".join(\"%.1f %s\" % coef for coef in coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We've so far done the following\n",
    "* Explored using sklearn's functions to build and predict models\n",
    "* Developed our own implementation\n",
    "* Added categorical features to our model features\n",
    "\n",
    "Let's see how the score changes as we try different transformations of our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1) Using patsy, create a model using the entire data set of X and y on the following formula:**\n",
    "**`sl ~ sx + yr + yd`**\n",
    "\n",
    "Create the model, fit, and report the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Let's plot out a scatter of yd on the x-axis and sl on the y-axis. We had done this earlier, but let's revisit the scatter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3) Hm...it's somewhat linear up until 20 years since graduating, and seems more to flatten out afterwards. Let's try a transformation that will help capture this relationship. Scatter np.power(yd,1.0/3.0) on the x axis, and sl on the y-axis. Compare with the original scatter**\n",
    "\n",
    "By taking the cube root of yd, the relationship between yd and sl might be more linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4) The relationship between the cube root of yd and sl looks more linear than the original. Add this transformation to your dataframe. Call it `cubedyd`. Let's try building a new model of `sl ~ sx + yr + cubedyd`.**\n",
    "\n",
    "Build the model and report the resulting score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Your score should have improved from 58% to 60%.\n",
    "\n",
    "We want to try one more thing. Try fitting and building a new model:\n",
    "\n",
    "`sl ~ sx + yr + cubedyd + np.random.randn(52) + np.random.random(52)`.\n",
    "\n",
    "**5) What is your score now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, your score should have increased from 60% to a higher $R^2$. Even though we added noise terms to our model, we fit non-zero coefficients to them. This is what is meant by 'gaming' $R^2$--just by adding terms, we were able to increase the score of our model.\n",
    "\n",
    "As a thought exercise, when there are 52 rows, all we need are 52 columns to get a square matrix and a perfect solution. i.e. $R^2$ = 1. \n",
    "\n",
    "** So we have a problem. We're sure that using the transformed `yd` factor should have increased our $R^2$. But, we also just saw that adding pure noise also increases our $R^2$. How do we get a measure of $R^2$ that isn't gamed just by adding additional factors?**\n",
    "\n",
    "**Solution** Adjusted $R^2$. The Adjusted $R^2$ is similar, but also penalizes for adding extraneous factors.\n",
    "\n",
    "(More here https://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2) \n",
    "\n",
    "To get Adjusted R2, lets use `statsmodel`, which provides much more information for regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### statsmodels\n",
    "\n",
    "- Statsmodels is a relatively new package that provides convenient utilities for investigating the results of a model. \n",
    "- It uses `patsy` to provide R formula syntax\n",
    "\n",
    "A formula allows you to write a functional relationship between variables.  \n",
    "Example:\n",
    "```R\n",
    "Y ~ X1 + X2 + X3\n",
    "```\n",
    "\n",
    "It automatically assumes there  is an intercept term. You can make this explicit by using \n",
    "\n",
    "```R\n",
    "Y ~ 1 + X1 + X2 + X3\n",
    "```\n",
    "\n",
    "http://statsmodels.sourceforge.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.ols(formula=\"sl ~ yr\", data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use categorical features as well, and `statsmodels` will use `patsy` for that just as we have been before. i.e. dont need to manually create your own design matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.ols(formula=\"sl ~ sx + yr\", data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sm.ols(formula=\"sl ~ sx + yr + rk\", data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see `+` is not acting as an addition operator but as a separator between other variables.\n",
    "\n",
    "There are other operators that lose their algebraic meaning in a formula. \n",
    "\n",
    "- `+` adds a new variable.\n",
    "- `:` adds the _interaction_ of two variables. \n",
    "- `*` adds the original terms as well as their interaction effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding an interaction effect\n",
    "model = sm.ols(\" sl ~ sx*rk\", data).fit() #This is like sl ~ sx + rk + sx*rk\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can also convert a numerical field to a categorical variable, if you think that might be a better representation.\n",
    "- Please be aware though that changing numerical values into categories will increase the number of features in your dataset, as each category will have its own column.\n",
    "- This will increase the complexity of the model, and hence your chances of **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting a field to a categorical variable\n",
    "model = sm.ols(\" sl ~ sx*rk + C(yr)\", data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization library `seaborn` includes some linear regression functionality as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scatter plots, seaborn can add the trendline and confidence bounds automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = sb.lmplot(x = 'yd', y='sl', data=data)\n",
    "f = plt.title(\"Salary vs years since degree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = sb.lmplot(x = 'yd', y='sl', col='sx', data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = sb.lmplot(x = 'yr', y='sl', col='rk', row='sx', data=data)\n",
    "f = plt.ylim(0, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we're a little short on data to make these last plots convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we haven't looked at cross-validation yet, while supervised problems, like linear regression, always require a training and a testset.\n",
    "- However, if you have a lot of data, and not so many features, trying to fit all these samples on a single line (or hyperplane in more dimensions) is a strong example of regularization already.\n",
    "- With many features, few samples, or a complex model (e.g., polynomial regression), overfitting is very likely, though, and not only need you to cross-validate your model, also you need to introduce regularization to make your model simpler.\n",
    "\n",
    "We will explore regularization and more into cross-validation after the break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Further reading\n",
    "\n",
    "- [Linear Regression with Python](http://connor-johnson.com/2014/02/18/linear-regression-with-python/)\n",
    "- [Statsmodels Documentation](http://statsmodels.sourceforge.net/stable/index.html)\n",
    "- [Python 538 Model](https://github.com/jseabold/538model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we'll use one of sklearn's standard datasets to analyze Boston house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "features = boston.feature_names\n",
    "# print boston.DESCR  # print this to get information on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's make a histogram of all house prices in the dataset. \n",
    "- It's a little unclear in what units the prices are, let's assume it's in $K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find a linear model that fits the data, and with which you feel comfortable.\n",
    "- Compute the average $R^2$ of the model using cross validation sklearn.cross_validation cross_val_score with cv=10.\n",
    "- (*) Compute the MSE of the model as well. Remember that $MSE = \\frac{1}{N}\\sum{{(y_{pred}-y)}^2}$\n",
    "- Prices often behave expontential, rather than linear (people often say +2%, rather than +\\$200). Let's try modeling the logarithm of the price and see if that improves your model: `y = np.log(boston.target)` \n",
    "\n",
    "- (*) How much more worth would a house be in a crime-free community, compared to the same house in a community with a crime rate of 6 per resident? (Note that this might not be a fair analysis as features might correlate with themselves  as well.)\n",
    "- (**) What would be a good way of analyzing which feature has the biggest influence on the house price?\n",
    "\n",
    "- (\\*\\*\\*) Let's also examine the effect of outliers. Taking the entire X and entire y dataset, try fitting a model and report its score and coefficients. Then, multiply one of the values by 100000000 (or some commensurately high value). Now refit and report its new score and coefficients. How much did the coefficients change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# What's Next?\n",
    "\n",
    "Things we didn't cover, but will next time:\n",
    "* Having linearly dependent columns. What happens when theres multiple very similar columns? How do the coefficients vary? \n",
    "* Using linear regression for weight analysis. \n",
    "* Regularization of beta coefficients for better out of sample prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
