{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS vs. Ridge vs. LASSO\n",
    "\n",
    "The usage of ridge and lasso regressions are very similar to the linear regressions. To motivate the initial usage of these models, we are going to create a high degree dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quadFunc(x): return 1 + .1*(x-4) ** 2\n",
    "\n",
    "def noiseFunc(x): return (np.random.random(len(x))-0.5)\n",
    "\n",
    "def noisyQuad(x): return quadFunc(x) + noiseFunc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = np.linspace(0,10,1000)\n",
    "y_true = quadFunc(domain)\n",
    "\n",
    "x_sample = np.sort(np.random.choice(domain,10))\n",
    "y_sample = noisyQuad(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(domain,quadFunc(domain),label=\"Truth\")\n",
    "plt.scatter(x_sample,y_sample,label=\"Sample\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Ground truth and Sample Noisy Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without seeing the ground truth line, we might believe that there is certainly some degree of polynomial effects here. Instead of the janky way of manually creating polynomials, we are going to use sklearn pipeline to create a 'model pipeline' using the PolynomialFeatures transformations.\n",
    "\n",
    "\n",
    "[Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) Let's you set up a pipeline of transformations, model building, etc.\n",
    "\n",
    "[Preprocessing](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) Will use the PolynomialFeatures within Preprocessing)\n",
    "\n",
    "[Metrics](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) Provides other scoring mechanisms, we will use the mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([x_sample]).T # Reshape x_sample into a matrix\n",
    "\n",
    "pf = preprocessing.PolynomialFeatures(degree=20)\n",
    "std = preprocessing.StandardScaler()\n",
    "model = pipeline.make_pipeline(pf,std,linear_model.LinearRegression())\n",
    "model.fit(X,y_sample)\n",
    "\n",
    "#Now predict over our domain so we can plot it\n",
    "y_pred = model.predict( np.array([domain]).T )\n",
    "r2_score = model.score(np.array([domain]).T,y_true)\n",
    "mse_score = metrics.mean_squared_error(y_true,y_pred)\n",
    "\n",
    "#Plot the ground truth, data points, and our predictions over the domain\n",
    "plt.plot(domain,y_true,label=\"Truth\")\n",
    "plt.scatter(x_sample,y_sample,label=\"Sample\")\n",
    "plt.plot(domain,y_pred,label=\"Pred\")\n",
    "plt.title(\"OLS, R2: %.2f MSE: %.2f\" % (r2_score, mse_score))\n",
    "plt.xlim(-2,12), plt.ylim(0,5)\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OLS Coefficient Diagnostics\n",
    "\n",
    "#Each step of the 'pipeline' model is saved as a list of tuples\n",
    "model.steps\n",
    "\n",
    "coefs = model.steps[2][1].coef_\n",
    "coefs = np.insert(coefs,0,model.steps[2][1].intercept_)\n",
    "nz    = np.sum( coefs != 0 )\n",
    "csum  = np.sum( np.abs( coefs ) )\n",
    "\n",
    "plt.plot(coefs)\n",
    "plt.title(\"OLS, NZ Coefs: %d, CSum: %.2f\"% (nz,csum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([x_sample]).T # Reshape x_sample into a matrix\n",
    "\n",
    "pf = preprocessing.PolynomialFeatures(degree=20)\n",
    "std = preprocessing.StandardScaler()\n",
    "model = pipeline.make_pipeline(pf,std,linear_model.Ridge(alpha=1.0))\n",
    "model.fit(X,y_sample)\n",
    "\n",
    "#Now predict over our domain so we can plot it\n",
    "y_pred = model.predict( np.array([domain]).T )\n",
    "\n",
    "r2_score = model.score(np.array([domain]).T,y_true)\n",
    "mse_score = metrics.mean_squared_error(y_true,y_pred)\n",
    "\n",
    "\n",
    "plt.plot(domain,y_true,label=\"Truth\")\n",
    "plt.scatter(x_sample,y_sample,label=\"Sample\")\n",
    "plt.plot(domain,y_pred,label=\"Pred\")\n",
    "plt.xlim(-2,12), plt.ylim(0,5)\n",
    "plt.title(\"Ridge, R2: %.2f MSE: %.2f\" % (r2_score, mse_score))\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ridge Coefficient Diagnostics\n",
    "\n",
    "coefs = model.steps[2][1].coef_\n",
    "coefs = np.insert(coefs,0,model.steps[2][1].intercept_)\n",
    "nz    = np.sum( coefs != 0 )\n",
    "csum  = np.sum( np.abs( coefs ) )\n",
    "\n",
    "plt.plot(coefs)\n",
    "plt.title(\"Ridge, NZ Coefs: %d, CSum: %.2f\"% (nz,csum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([x_sample]).T # Reshape x_sample into a matrix\n",
    "\n",
    "pf = preprocessing.PolynomialFeatures(degree=20)\n",
    "std = preprocessing.StandardScaler()\n",
    "model = pipeline.make_pipeline(pf,std,linear_model.Lasso(alpha=.1))\n",
    "model.fit(X,y_sample)\n",
    "\n",
    "#Now predict over our domain so we can plot it\n",
    "y_pred = model.predict( np.array([domain]).T )\n",
    "\n",
    "r2_score = model.score(np.array([domain]).T,y_true)\n",
    "mse_score = metrics.mean_squared_error(y_true,y_pred)\n",
    "\n",
    "plt.plot(domain,y_true,label=\"Truth\")\n",
    "plt.scatter(x_sample,y_sample,label=\"Sample\")\n",
    "plt.plot(domain,y_pred,label=\"Pred\")\n",
    "plt.xlim(-2,12), plt.ylim(0,5)\n",
    "plt.title(\"LASSO, R2: %.2f MSE: %.2f\" % (r2_score, mse_score))\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LASSOCoefficient Diagnostics\n",
    "\n",
    "coefs = model.steps[2][1].coef_\n",
    "coefs = np.insert(coefs,0,model.steps[2][1].intercept_)\n",
    "nz    = np.sum(coefs != 0 )\n",
    "csum  = np.sum( np.abs( coefs ) )\n",
    "\n",
    "\n",
    "plt.plot(coefs)\n",
    "plt.title(\"LASSO, NZ Coefs: %d, CSum: %.2f\"% (nz, csum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Holy, Ridge and LASSO regression perform much better than OLS when data is sparse.**\n",
    "\n",
    "As you can see, for the examples, both Ridge and LASSO have similar 'complexity' in terms of beta sums, but LASSO ALSO zeroed out many of these coefficients. This is automatic feature selection--you can describe your model with far fewer features than you need to with Ridge.\n",
    "\n",
    "There's a few issues though. For one, I had to remember to preprocess the data to standardize the features so that each coefficient can be on the same \"scale\" when reducing.\n",
    "\n",
    "I've manually chosen the hyperparameter--in this case, \"alpha\", which controls the amount of reduction in beta sizes.\n",
    "\n",
    "Ideally, we want to automatically choose the hyperparameters through cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO / Ridge Cross Validation\n",
    "\n",
    "The LassoCV() and RidgeCV() classes automatically choose the best lambda/alpha value for you through cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the space of predictions as well as generate a few samples\n",
    "def quadFunc(x): return 1 + .5*(x-4) ** 2 - 12.0*np.sin(x) + 4*np.log(x+.2) - .1*(x-1)**3\n",
    "\n",
    "def noiseFunc(x): return (10.0*(np.random.random(len(x))-0.5))\n",
    "\n",
    "def noisyQuad(x): return quadFunc(x) + noiseFunc(x)\n",
    "\n",
    "# Create the \"True\" function\n",
    "domain = np.linspace(0,10,1000)\n",
    "y_true = quadFunc(domain)\n",
    "\n",
    "# Generate points using a noisy version of the function\n",
    "x_sample = np.sort(np.random.choice(domain,50))\n",
    "y_sample = noisyQuad(x_sample)\n",
    "\n",
    "# Plot the points vs truth\n",
    "plt.plot(domain,y_true)\n",
    "plt.scatter(x_sample,y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([x_sample]).T\n",
    "X_test = np.array([domain]).T\n",
    "\n",
    "\n",
    "pf = preprocessing.PolynomialFeatures(degree=20)\n",
    "std = preprocessing.StandardScaler()\n",
    "model = pipeline.make_pipeline(pf,std,linear_model.LassoCV(cv=10))\n",
    "model.fit(X,y_sample)\n",
    "\n",
    "alphas   = model.steps[2][1].alphas_\n",
    "alpha    = model.steps[2][1].alpha_\n",
    "mse_path = model.steps[2][1].mse_path_\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.log10(alphas), mse_path,\":\")\n",
    "plt.plot(np.log10(alphas), np.mean(mse_path,axis=-1),'k',linewidth=2)\n",
    "plt.axvline(np.log10(alpha))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(domain, model.predict(X_test),label=\"Pred\")\n",
    "plt.plot(domain, y_true,label=\"Truth\")\n",
    "plt.scatter(x_sample,y_sample,label=\"Data\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice how noisy each individual path is! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, we will be comparing OLS vs Ridge vs LASSO for the purpose of stock return prediction. The data is scaled down from a popular, currently ongoing Kaggle Competition\n",
    "\n",
    "Using feature_x where x is between 1 and 25, as well as Ret_MinusOne (return day minus one), and Ret_MinusTwo (return day minus two), try to predict the next day's return, Ret_PlusOne.\n",
    "\n",
    "**Helpful Notes**\n",
    "\n",
    "* Try using the return features as well as non-integer features at first. They make no mention of whether features such as Feature_5, Feature_9, Feature_10 are categorical or if they are ordered (perhaps deciles).\n",
    "* Use the cross_validation_score module from sklearn to run cross validation with cv=KFold [Link](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html). Choose shuffle=True in order to shuffle the rows for cross validation\n",
    "* Try winsorizing the features as well as response variable! If you plot a histogram of our response variable, Ret_PlusOne, for instance, you will see that there are a few outliers that really screw up the histogram. Trying to fit OLS on these outliers for instance will bias our results (remember that OLS: min (y - y_pred)^2 )\n",
    "* Lastly, **ask questions!** This exercise is very free form--it is meant to force you to write code and for you to find out what you understand and what you don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_repo = \"/Users/brianchung/Desktop/ga-ds/\"\n",
    "data = pd.read_csv(path_to_repo + \"/06_regularization/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
